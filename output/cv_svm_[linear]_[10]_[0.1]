
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(e1071);
> library(randomForest);
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
> library(topicmodels);
> #Normalize features of dataset
> normalize<-function(data){
+ 	cols<-ncol(data);
+ 	for(i in 1:cols){
+ 		if(is.numeric(data[,i]))data[,i]<-data[,i]/sum(data[,i]);
+ 	}
+ 	return(data);
+ }
> #Randomize the rows of dataset
> randomize<-function(data){
+ 	length<-nrow(data);
+ 	indices<-1:length;
+ 	rindices<-sample(indices,length,replace=F);
+ 	data<-data[rindices,];
+ 	return(data);
+ }
> #This function does cross-validation
> #input data	datamatrix
> #input method	randomForest,SVM,naiveBayes,etc
> #input K	K-foldvalidation
> #return		confusion matrices of each fold
> cross_validation<-function(data,method,K=10){
+ 	#Calculate matrix length and size
+ 	length<-nrow(data);
+ 	partitionSize<-floor(length/K);
+ 	#Confusion Matrix
+ 	confusionMatrices<-list();
+ 	#List of models
+ 	models<-list()
+ 	#Normalize Data
+ 	data<-normalize(data);
+ 	#Shuffle Data
+ 	randomData<-randomize(data);
+ 	#Perform Cross-Validation
+ 	for(k in 1:K){
+ 		# Debug
+ 		print(sprintf('K-Fold: %d',k));
+ 		#Get Traning Set
+ 		startIdx<-(k-1)*partitionSize+1;
+ 		indices<-startIdx:(startIdx+partitionSize-1);
+ 		trainset<-randomData[-indices,];
+ 		testset<-randomData[indices,];
+ 		#Train Data
+ 		ifelse(method=='svm',
+ 			model<-svm(class~.,data=trainset,kernel=svm.kernel,
+ 				gamma=svm.gamma,cost=svm.cost),
+ 			ifelse(method=='naiveBayes',
+ 				model<-naiveBayes(class~.,data=trainset),
+ 				model<-randomForest(class~.,data=trainset,mtry=rf.mtry)
+ 		));
+ 		prediction<-predict(model,testset);
+ 		#Construct Confusion Matrix
+ 		tab<-table(actual=testset[,1],predicted=prediction);
+ 		confusionMatrices[[k]]<-tab;
+ 		models[[k]]<-model;
+ 	}
+ 	return(list(confusionMatrices=confusionMatrices,models=models));
+ }
> #This function produce per class performance matrix from confusion matrix
> #input matrix confusionmatrix
> measure_performance<-function(mat){
+ 	#Get Size of Matrix
+ 	size<-nrow(mat);
+ 	#Initialize Result
+ 	result<-matrix(,nrow=size,ncol=6);
+ 	col_names<-c('TP','FN','FP','Recall','Precision','F-measure');
+ 	row_names<-rownames(mat);
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Iterate over all class
+ 	for(c in 1:size){
+ 	#Calculate Performance Metrics
+ 		TP<-mat[c,c];
+ 		FN<-sum(mat[c,-c]);
+ 		FP<-sum(mat[-c,c]);
+ 		recall<-TP/(TP+FN);
+ 		precision<-TP/(TP+FP);
+ 		fmeasure<-(2*precision*recall)/(precision+recall);
+ 		#SaveToResult
+ 		result[c,1]<-TP;result[c,2]<-FN;result[c,3]<-FP;
+ 		result[c,4]<-recall;result[c,5]<-precision;
+ 		result[c,6]<-fmeasure;
+ 	}
+ 	return(result);
+ }
> #This function produce the overall performance matrix
> #input matrix per class performance matrix
> #return over all performance matrix
> get_overall_performance<-function(mat){
+ 	result<-matrix(0,nrow=2,ncol=3);
+ 	col_names<-c('Recall','Precision','F-measure');
+ 	row_names<-c('micro-avg','macro-avg');
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Micro Averaging
+ 	TP<-sum(mat[,1]);
+ 	FN<-sum(mat[,2]);
+ 	FP<-sum(mat[,3]);
+ 	result[1,1]<-TP/(TP+FN);
+ 	result[1,2]<-TP/(TP+FP);
+ 	result[1,3]<-2*result[1,2]*result[1,1]/(result[1,1]+result[1,2]);
+ 	#Macro Averaging
+ 	result[2,1]<-mean(mat[,4],na.rm=T);
+ 	result[2,2]<-mean(mat[,5],na.rm=T);
+ 	result[2,3]<-mean(mat[,6],na.rm=T);
+ 	return(result);
+ }
> # Fetch Command Line Arguments
> args <- commandArgs(trailingOnly=T)
> ifelse(length(args) > 1, method<-args[1], method<-'naiveBayes')
[1] "svm"
> print(sprintf("Method: %s",method))
[1] "Method: svm"
> if(method == 'svm'){
+ 	if(length(args) == 4){
+ 		svm.kernel = args[2]
+ 		svm.gamma = as.numeric(args[3])
+ 		svm.cost = as.numeric(args[4])
+ 	}
+ 	else{
+ 		svm.kernel='radial'
+ 		svm.gamma = 0.01
+ 		svm.cost = 10000
+ 	}
+ 	print(sprintf("Settings: Kernel:%s Gamma:%f Cost:%f",
+ 		svm.kernel,svm.gamma,svm.cost))
+ }
[1] "Settings: Kernel:linear Gamma:10.000000 Cost:0.100000"
> if(method == 'randomForest'){
+ 	if(length(args) == 2){
+ 		rf.mtry = as.numeric(args[2])
+ 	}
+ 	else{
+ 		rf.mtry = 4
+ 	}
+ 	print(sprintf("Settings: mtry:%f",rf.mtry))
+ }
> 
> #Prepare Data
> data<-read.csv('./doc_tags.csv',header=T);
> load('./lda.Rdata');
> load('./dtm.Rdata');
> 
> print('Preparing Data.')
[1] "Preparing Data."
> # Add prefix to feature names
> colnames(dtm)<-paste0('feature.',colnames(dtm))
> # Convert topic models to strings
> doc.topics<-sapply(topics(lda),as.character);
> 
> # Extract the purpose and tags column
> data<-data[,colnames(data) %in% c('purpose','tags')];
> # Combine Tags with features
> data<-cbind(data,doc.topics,dtm);
> 
> # Preparing class column
> colnames(data)[2]<-'class'; # Replace Class Name
> data$class<-sapply(data$class,as.character); # Convert to characters
> data$class<-factor(data$class); # Factorize class
> 
> # Convert to data frame
> data<-as.data.frame(data);
> 
> #Separate Training and Testing Set
> train_set<-subset(data,purpose == 'train'); # Get Training Set
> test_set<-subset(data,purpose == 'test'); # Get Training Set
> train_set<-train_set[,2:dim(data)[2]]; # Remove Purpose Column
> test_set<-test_set[,2:dim(data)[2]]; # Remove Purpose Column
> #Do cross validation
> K<-10;
> print(sprintf('Starting %d-fold cross validation',K));
[1] "Starting 10-fold cross validation"
> result<-cross_validation(train_set,method,K=K);
[1] "K-Fold: 1"
[1] "K-Fold: 2"
[1] "K-Fold: 3"
[1] "K-Fold: 4"
[1] "K-Fold: 5"
[1] "K-Fold: 6"
[1] "K-Fold: 7"
[1] "K-Fold: 8"
[1] "K-Fold: 9"
[1] "K-Fold: 10"
> confusionMatrices<-result$confusionMatrices;
> models<-result$models;
> 
> #Start Evaluation
> print('Evaluation on training set')
[1] "Evaluation on training set"
> #Do Micro Averaging
> overallConfusionMatrix<-confusionMatrices[[1]];
> for(i in 2:length(confusionMatrices)){
+ 	overallConfusionMatrix<-overallConfusionMatrix+confusionMatrices[[i]];
+ }
> accuracy<-sum(diag(overallConfusionMatrix))/sum(overallConfusionMatrix);
> performanceMatrix<-measure_performance(overallConfusionMatrix);
> print("Result of Micro-Averaging");
[1] "Result of Micro-Averaging"
> print(performanceMatrix);
      TP  FN  FP     Recall  Precision  F-measure
110  117  77  63 0.60309278 0.65000000 0.62566845
128  242  83  67 0.74461538 0.78317152 0.76340694
132    5 194 109 0.02512563 0.04385965 0.03194888
2   1383 103 168 0.93068641 0.89168279 0.91076720
21     6 155 102 0.03726708 0.05555556 0.04460967
30   246  98  82 0.71511628 0.75000000 0.73214286
37  2582 131 117 0.95171397 0.95665061 0.95417591
47   170 228 371 0.42713568 0.31423290 0.36208733
57   141 140  92 0.50177936 0.60515021 0.54863813
75   321 128 166 0.71492205 0.65913758 0.68589744
> print(get_overall_performance(performanceMatrix));
             Recall Precision F-measure
micro-avg 0.7958779 0.7958779 0.7958779
macro-avg 0.5651455 0.5709441 0.5659343
> print(sprintf("Accuracy:%f",accuracy));
[1] "Accuracy:0.795878"
> #Do Macro Averaging
> accuracies<-1:K*0;
> overallPerformanceMatrix<-measure_performance(confusionMatrices[[1]]);
> accuracies[1]<-sum(diag(confusionMatrices[[1]]))/sum(confusionMatrices[[1]]);
> for(i in 2:length(confusionMatrices)){
+ 	performanceMatrix<-measure_performance(confusionMatrices[[i]]);
+ 	overallPerformanceMatrix<-overallPerformanceMatrix+performanceMatrix;
+ 	accuracies[i]<-sum(diag(confusionMatrices[[i]]))/sum(confusionMatrices[[i]]);
+ }
> overallPerformanceMatrix[,4:6]<-overallPerformanceMatrix[,4:6]/K;
> print("Result of Macro-Averaing");
[1] "Result of Macro-Averaing"
> print(overallPerformanceMatrix);
      TP  FN  FP     Recall  Precision F-measure
110  117  77  63 0.61838111 0.65585949 0.6276634
128  242  83  67 0.74380717 0.78373715 0.7622427
132    5 194 109 0.02547980 0.03733516       NaN
2   1383 103 168 0.93071764 0.89270100 0.9109210
21     6 155 102 0.04383195 0.05106283       NaN
30   246  98  82 0.72317316 0.75165319 0.7343719
37  2582 131 117 0.95165880 0.95683631 0.9540565
47   170 228 371 0.42349652 0.31597023 0.3593389
57   141 140  92 0.51294265 0.60307794 0.5447949
75   321 128 166 0.71535074 0.66255450 0.6845589
> print(get_overall_performance(overallPerformanceMatrix));
             Recall Precision F-measure
micro-avg 0.7958779 0.7958779 0.7958779
macro-avg 0.5688840 0.5710788 0.6972435
> std_accuracy<-sqrt(var(accuracies));
> mean_accuracy<-mean(accuracies);
> print(sprintf("MeanAccuracy=%f",mean_accuracy));
[1] "MeanAccuracy=0.795878"
> print(sprintf("99%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-2.58*std_accuracy,
+ 	mean_accuracy+2.58*std_accuracy));
[1] "99% ConfidenceInterval[0.750431,0.841325]"
> print(sprintf("95%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.96*std_accuracy,
+ 	mean_accuracy+1.96*std_accuracy));
[1] "95% ConfidenceInterval[0.761352,0.830403]"
> print(sprintf("90%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.64*std_accuracy,
+ 	mean_accuracy+1.64*std_accuracy));
[1] "90% ConfidenceInterval[0.766989,0.824767]"
> 
> #Evaluation on test set
> print('Evaluation on test set')
[1] "Evaluation on test set"
> #Find best model
> accuracy_rank<-rank(-accuracies);
> best_model<-models[accuracy_rank==1][[1]];
> #Classification on test set
> prediction<-predict(best_model,test_set);
> #Construct Confusion Matrix
> confusionMatrix<-table(actual=test_set[,1],predicted=prediction);
> accuracy<-sum(diag(confusionMatrix))/sum(confusionMatrix);
> print(confusionMatrix)
      predicted
actual 110 128 132   2  21  30  37  47  57  75
   110  27   0   0  40   0  12   7   0   0   1
   128  11  12   0  55   1   5  20   3   0   1
   132  20   7   0  11   7   6   5   9   0   1
   2    10   0   0 545   0   3  86   0   0   0
   21   11   5   0  10   6   4   3   8   0   1
   30   15   1   0  87   2  21  37   1   0   0
   37    7   8   0  42   0   4 988   0   0   0
   47   33  12   0  26  12  11  18  19   0   3
   57    6   7   0  38   1   0  40   1   6   1
   75   14  11   0  68   0   1  41   4   0   1
> print(sprintf('Test Set: Accuracy=%f',accuracy))
[1] "Test Set: Accuracy=0.639764"
> 
> proc.time()
   user  system elapsed 
379.047   3.105 449.440 
