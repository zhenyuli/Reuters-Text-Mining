
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(e1071);
> library(randomForest);
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
> library(topicmodels);
> #Normalize features of dataset
> normalize<-function(data){
+ 	cols<-ncol(data);
+ 	for(i in 1:cols){
+ 		if(is.numeric(data[,i]))data[,i]<-data[,i]/sum(data[,i]);
+ 	}
+ 	return(data);
+ }
> #Randomize the rows of dataset
> randomize<-function(data){
+ 	length<-nrow(data);
+ 	indices<-1:length;
+ 	rindices<-sample(indices,length,replace=F);
+ 	data<-data[rindices,];
+ 	return(data);
+ }
> #This function does cross-validation
> #input data	datamatrix
> #input method	randomForest,SVM,naiveBayes,etc
> #input K	K-foldvalidation
> #return		confusion matrices of each fold
> cross_validation<-function(data,method,K=10){
+ 	#Calculate matrix length and size
+ 	length<-nrow(data);
+ 	partitionSize<-floor(length/K);
+ 	#Confusion Matrix
+ 	confusionMatrices<-list();
+ 	#List of models
+ 	models<-list()
+ 	#Normalize Data
+ 	data<-normalize(data);
+ 	#Shuffle Data
+ 	randomData<-randomize(data);
+ 	#Perform Cross-Validation
+ 	for(k in 1:K){
+ 		# Debug
+ 		print(sprintf('K-Fold: %d',k));
+ 		#Get Traning Set
+ 		startIdx<-(k-1)*partitionSize+1;
+ 		indices<-startIdx:(startIdx+partitionSize-1);
+ 		trainset<-randomData[-indices,];
+ 		testset<-randomData[indices,];
+ 		#Train Data
+ 		ifelse(method=='svm',
+ 			model<-svm(class~.,data=trainset,kernel=svm.kernel,
+ 				gamma=svm.gamma,cost=svm.cost),
+ 			ifelse(method=='naiveBayes',
+ 				model<-naiveBayes(class~.,data=trainset),
+ 				model<-randomForest(class~.,data=trainset,mtry=rf.mtry)
+ 		));
+ 		prediction<-predict(model,testset);
+ 		#Construct Confusion Matrix
+ 		tab<-table(actual=testset[,1],predicted=prediction);
+ 		confusionMatrices[[k]]<-tab;
+ 		models[[k]]<-model;
+ 	}
+ 	return(list(confusionMatrices=confusionMatrices,models=models));
+ }
> #This function produce per class performance matrix from confusion matrix
> #input matrix confusionmatrix
> measure_performance<-function(mat){
+ 	#Get Size of Matrix
+ 	size<-nrow(mat);
+ 	#Initialize Result
+ 	result<-matrix(,nrow=size,ncol=6);
+ 	col_names<-c('TP','FN','FP','Recall','Precision','F-measure');
+ 	row_names<-rownames(mat);
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Iterate over all class
+ 	for(c in 1:size){
+ 	#Calculate Performance Metrics
+ 		TP<-mat[c,c];
+ 		FN<-sum(mat[c,-c]);
+ 		FP<-sum(mat[-c,c]);
+ 		recall<-TP/(TP+FN);
+ 		precision<-TP/(TP+FP);
+ 		fmeasure<-(2*precision*recall)/(precision+recall);
+ 		#SaveToResult
+ 		result[c,1]<-TP;result[c,2]<-FN;result[c,3]<-FP;
+ 		result[c,4]<-recall;result[c,5]<-precision;
+ 		result[c,6]<-fmeasure;
+ 	}
+ 	return(result);
+ }
> #This function produce the overall performance matrix
> #input matrix per class performance matrix
> #return over all performance matrix
> get_overall_performance<-function(mat){
+ 	result<-matrix(0,nrow=2,ncol=3);
+ 	col_names<-c('Recall','Precision','F-measure');
+ 	row_names<-c('micro-avg','macro-avg');
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Micro Averaging
+ 	TP<-sum(mat[,1]);
+ 	FN<-sum(mat[,2]);
+ 	FP<-sum(mat[,3]);
+ 	result[1,1]<-TP/(TP+FN);
+ 	result[1,2]<-TP/(TP+FP);
+ 	result[1,3]<-2*result[1,2]*result[1,1]/(result[1,1]+result[1,2]);
+ 	#Macro Averaging
+ 	result[2,1]<-mean(mat[,4],na.rm=T);
+ 	result[2,2]<-mean(mat[,5],na.rm=T);
+ 	result[2,3]<-mean(mat[,6],na.rm=T);
+ 	return(result);
+ }
> # Fetch Command Line Arguments
> args <- commandArgs(trailingOnly=T)
> ifelse(length(args) > 1, method<-args[1], method<-'naiveBayes')
[1] "svm"
> print(sprintf("Method: %s",method))
[1] "Method: svm"
> if(method == 'svm'){
+ 	if(length(args) == 4){
+ 		svm.kernel = args[2]
+ 		svm.gamma = as.numeric(args[3])
+ 		svm.cost = as.numeric(args[4])
+ 	}
+ 	else{
+ 		svm.kernel='radial'
+ 		svm.gamma = 0.01
+ 		svm.cost = 10000
+ 	}
+ 	print(sprintf("Settings: Kernel:%s Gamma:%f Cost:%f",
+ 		svm.kernel,svm.gamma,svm.cost))
+ }
[1] "Settings: Kernel:linear Gamma:10.000000 Cost:0.010000"
> if(method == 'randomForest'){
+ 	if(length(args) == 2){
+ 		rf.mtry = as.numeric(args[2])
+ 	}
+ 	else{
+ 		rf.mtry = 4
+ 	}
+ 	print(sprintf("Settings: mtry:%f",rf.mtry))
+ }
> 
> #Prepare Data
> data<-read.csv('./doc_tags.csv',header=T);
> load('./lda.Rdata');
> load('./dtm.Rdata');
> 
> print('Preparing Data.')
[1] "Preparing Data."
> # Add prefix to feature names
> colnames(dtm)<-paste0('feature.',colnames(dtm))
> # Convert topic models to strings
> doc.topics<-sapply(topics(lda),as.character);
> 
> # Extract the purpose and tags column
> data<-data[,colnames(data) %in% c('purpose','tags')];
> # Combine Tags with features
> data<-cbind(data,doc.topics,dtm);
> 
> # Preparing class column
> colnames(data)[2]<-'class'; # Replace Class Name
> data$class<-sapply(data$class,as.character); # Convert to characters
> data$class<-factor(data$class); # Factorize class
> 
> # Convert to data frame
> data<-as.data.frame(data);
> 
> #Separate Training and Testing Set
> train_set<-subset(data,purpose == 'train'); # Get Training Set
> test_set<-subset(data,purpose == 'test'); # Get Training Set
> train_set<-train_set[,2:dim(data)[2]]; # Remove Purpose Column
> test_set<-test_set[,2:dim(data)[2]]; # Remove Purpose Column
> #Do cross validation
> K<-10;
> print(sprintf('Starting %d-fold cross validation',K));
[1] "Starting 10-fold cross validation"
> result<-cross_validation(train_set,method,K=K);
[1] "K-Fold: 1"
[1] "K-Fold: 2"
[1] "K-Fold: 3"
[1] "K-Fold: 4"
[1] "K-Fold: 5"
[1] "K-Fold: 6"
[1] "K-Fold: 7"
[1] "K-Fold: 8"
[1] "K-Fold: 9"
[1] "K-Fold: 10"
> confusionMatrices<-result$confusionMatrices;
> models<-result$models;
> 
> #Start Evaluation
> print('Evaluation on training set')
[1] "Evaluation on training set"
> #Do Micro Averaging
> overallConfusionMatrix<-confusionMatrices[[1]];
> for(i in 2:length(confusionMatrices)){
+ 	overallConfusionMatrix<-overallConfusionMatrix+confusionMatrices[[i]];
+ }
> accuracy<-sum(diag(overallConfusionMatrix))/sum(overallConfusionMatrix);
> performanceMatrix<-measure_performance(overallConfusionMatrix);
> print("Result of Micro-Averaging");
[1] "Result of Micro-Averaging"
> print(performanceMatrix);
      TP  FN  FP     Recall Precision  F-measure
110  131  63  60 0.67525773 0.6858639 0.68051948
128  261  63  60 0.80555556 0.8130841 0.80930233
132    0 199  42 0.00000000 0.0000000        NaN
2   1396  90 202 0.93943472 0.8735920 0.90531777
21     6 155  48 0.03726708 0.1111111 0.05581395
30   259  84  59 0.75510204 0.8144654 0.78366112
37  2574 141 103 0.94806630 0.9615241 0.95474777
47   269 128 366 0.67758186 0.4236220 0.52131783
57   151 131  80 0.53546099 0.6536797 0.58869396
75   337 112 146 0.75055679 0.6977226 0.72317597
> print(get_overall_performance(performanceMatrix));
             Recall Precision F-measure
micro-avg 0.8219847 0.8219847 0.8219847
macro-avg 0.6124283 0.6034665 0.6691722
> print(sprintf("Accuracy:%f",accuracy));
[1] "Accuracy:0.821985"
> #Do Macro Averaging
> accuracies<-1:K*0;
> overallPerformanceMatrix<-measure_performance(confusionMatrices[[1]]);
> accuracies[1]<-sum(diag(confusionMatrices[[1]]))/sum(confusionMatrices[[1]]);
> for(i in 2:length(confusionMatrices)){
+ 	performanceMatrix<-measure_performance(confusionMatrices[[i]]);
+ 	overallPerformanceMatrix<-overallPerformanceMatrix+performanceMatrix;
+ 	accuracies[i]<-sum(diag(confusionMatrices[[i]]))/sum(confusionMatrices[[i]]);
+ }
> overallPerformanceMatrix[,4:6]<-overallPerformanceMatrix[,4:6]/K;
> print("Result of Macro-Averaing");
[1] "Result of Macro-Averaing"
> print(overallPerformanceMatrix);
      TP  FN  FP     Recall  Precision F-measure
110  131  63  60 0.68503825 0.68044986 0.6680714
128  261  63  60 0.79618866 0.80810268 0.7984713
132    0 199  42 0.00000000 0.00000000       NaN
2   1396  90 202 0.93998371 0.87351995 0.9051830
21     6 155  48 0.04075758 0.05535714       NaN
30   259  84  59 0.75757471 0.81779652 0.7827845
37  2574 141 103 0.94816709 0.96160904 0.9547668
47   269 128 366 0.67273679 0.42180723 0.5160447
57   151 131  80 0.53350806 0.65757442 0.5868408
75   337 112 146 0.75479806 0.70064942 0.7243518
> print(get_overall_performance(overallPerformanceMatrix));
             Recall Precision F-measure
micro-avg 0.8219847 0.8219847 0.8219847
macro-avg 0.6128753 0.5976866 0.7420643
> std_accuracy<-sqrt(var(accuracies));
> mean_accuracy<-mean(accuracies);
> print(sprintf("MeanAccuracy=%f",mean_accuracy));
[1] "MeanAccuracy=0.821985"
> print(sprintf("99%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-2.58*std_accuracy,
+ 	mean_accuracy+2.58*std_accuracy));
[1] "99% ConfidenceInterval[0.792090,0.851879]"
> print(sprintf("95%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.96*std_accuracy,
+ 	mean_accuracy+1.96*std_accuracy));
[1] "95% ConfidenceInterval[0.799274,0.844695]"
> print(sprintf("90%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.64*std_accuracy,
+ 	mean_accuracy+1.64*std_accuracy));
[1] "90% ConfidenceInterval[0.802982,0.840987]"
> 
> #Evaluation on test set
> print('Evaluation on test set')
[1] "Evaluation on test set"
> #Find best model
> accuracy_rank<-rank(-accuracies);
> best_model<-models[accuracy_rank==1][[1]];
> #Classification on test set
> prediction<-predict(best_model,test_set);
> #Construct Confusion Matrix
> confusionMatrix<-table(actual=test_set[,1],predicted=prediction);
> accuracy<-sum(diag(confusionMatrix))/sum(confusionMatrix);
> print(confusionMatrix)
      predicted
actual 110 128 132   2  21  30  37  47  57  75
   110  11   0   0  41   0  31   4   0   0   0
   128   5   9   0  54   1  25  13   0   0   1
   132  15   0   0  26  15   9   1   0   0   0
   2     4   0   0 564   0   0  76   0   0   0
   21   10   0   0  19   8   9   2   0   0   0
   30    1   0   0  88   0  46  29   0   0   0
   37    1   0   0  48   1   0 999   0   0   0
   47   29   0   0  53  21  22   9   0   0   0
   57    4   2   0  50   6   5  28   0   5   0
   75    4   2   0  78   4  16  34   0   0   2
> print(sprintf('Test Set: Accuracy=%f',accuracy))
[1] "Test Set: Accuracy=0.647244"
> 
> proc.time()
   user  system elapsed 
453.892   4.407 770.785 
