
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(e1071);
> library(randomForest);
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
> library(topicmodels);
> #Normalize features of dataset
> normalize<-function(data){
+ 	cols<-ncol(data);
+ 	for(i in 1:cols){
+ 		if(is.numeric(data[,i]))data[,i]<-data[,i]/sum(data[,i]);
+ 	}
+ 	return(data);
+ }
> #Randomize the rows of dataset
> randomize<-function(data){
+ 	length<-nrow(data);
+ 	indices<-1:length;
+ 	rindices<-sample(indices,length,replace=F);
+ 	data<-data[rindices,];
+ 	return(data);
+ }
> #This function does cross-validation
> #input data	datamatrix
> #input method	randomForest,SVM,naiveBayes,etc
> #input K	K-foldvalidation
> #return		confusion matrices of each fold
> cross_validation<-function(data,method,K=10){
+ 	#Calculate matrix length and size
+ 	length<-nrow(data);
+ 	partitionSize<-floor(length/K);
+ 	#Confusion Matrix
+ 	confusionMatrices<-list();
+ 	#List of models
+ 	models<-list()
+ 	#Normalize Data
+ 	data<-normalize(data);
+ 	#Shuffle Data
+ 	randomData<-randomize(data);
+ 	#Perform Cross-Validation
+ 	for(k in 1:K){
+ 		# Debug
+ 		print(sprintf('K-Fold: %d',k));
+ 		#Get Traning Set
+ 		startIdx<-(k-1)*partitionSize+1;
+ 		indices<-startIdx:(startIdx+partitionSize-1);
+ 		trainset<-randomData[-indices,];
+ 		testset<-randomData[indices,];
+ 		#Train Data
+ 		ifelse(method=='svm',
+ 			model<-svm(class~.,data=trainset,kernel=svm.kernel,
+ 				gamma=svm.gamma,cost=svm.cost),
+ 			ifelse(method=='naiveBayes',
+ 				model<-naiveBayes(class~.,data=trainset),
+ 				model<-randomForest(class~.,data=trainset,mtry=rf.mtry)
+ 		));
+ 		prediction<-predict(model,testset);
+ 		#Construct Confusion Matrix
+ 		tab<-table(actual=testset[,1],predicted=prediction);
+ 		confusionMatrices[[k]]<-tab;
+ 		models[[k]]<-model;
+ 	}
+ 	return(list(confusionMatrices=confusionMatrices,models=models));
+ }
> #This function produce per class performance matrix from confusion matrix
> #input matrix confusionmatrix
> measure_performance<-function(mat){
+ 	#Get Size of Matrix
+ 	size<-nrow(mat);
+ 	#Initialize Result
+ 	result<-matrix(,nrow=size,ncol=6);
+ 	col_names<-c('TP','FN','FP','Recall','Precision','F-measure');
+ 	row_names<-rownames(mat);
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Iterate over all class
+ 	for(c in 1:size){
+ 	#Calculate Performance Metrics
+ 		TP<-mat[c,c];
+ 		FN<-sum(mat[c,-c]);
+ 		FP<-sum(mat[-c,c]);
+ 		recall<-TP/(TP+FN);
+ 		precision<-TP/(TP+FP);
+ 		fmeasure<-(2*precision*recall)/(precision+recall);
+ 		#SaveToResult
+ 		result[c,1]<-TP;result[c,2]<-FN;result[c,3]<-FP;
+ 		result[c,4]<-recall;result[c,5]<-precision;
+ 		result[c,6]<-fmeasure;
+ 	}
+ 	return(result);
+ }
> #This function produce the overall performance matrix
> #input matrix per class performance matrix
> #return over all performance matrix
> get_overall_performance<-function(mat){
+ 	result<-matrix(0,nrow=2,ncol=3);
+ 	col_names<-c('Recall','Precision','F-measure');
+ 	row_names<-c('micro-avg','macro-avg');
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Micro Averaging
+ 	TP<-sum(mat[,1]);
+ 	FN<-sum(mat[,2]);
+ 	FP<-sum(mat[,3]);
+ 	result[1,1]<-TP/(TP+FN);
+ 	result[1,2]<-TP/(TP+FP);
+ 	result[1,3]<-2*result[1,2]*result[1,1]/(result[1,1]+result[1,2]);
+ 	#Macro Averaging
+ 	result[2,1]<-mean(mat[,4],na.rm=T);
+ 	result[2,2]<-mean(mat[,5],na.rm=T);
+ 	result[2,3]<-mean(mat[,6],na.rm=T);
+ 	return(result);
+ }
> # Fetch Command Line Arguments
> args <- commandArgs(trailingOnly=T)
> ifelse(length(args) > 1, method<-args[1], method<-'naiveBayes')
[1] "svm"
> print(sprintf("Method: %s",method))
[1] "Method: svm"
> if(method == 'svm'){
+ 	if(length(args) == 4){
+ 		svm.kernel = args[2]
+ 		svm.gamma = as.numeric(args[3])
+ 		svm.cost = as.numeric(args[4])
+ 	}
+ 	else{
+ 		svm.kernel='radial'
+ 		svm.gamma = 0.01
+ 		svm.cost = 10000
+ 	}
+ 	print(sprintf("Settings: Kernel:%s Gamma:%f Cost:%f",
+ 		svm.kernel,svm.gamma,svm.cost))
+ }
[1] "Settings: Kernel:linear Gamma:0.100000 Cost:0.010000"
> if(method == 'randomForest'){
+ 	if(length(args) == 2){
+ 		rf.mtry = as.numeric(args[2])
+ 	}
+ 	else{
+ 		rf.mtry = 4
+ 	}
+ 	print(sprintf("Settings: mtry:%f",rf.mtry))
+ }
> 
> #Prepare Data
> data<-read.csv('./doc_tags.csv',header=T);
> load('./lda.Rdata');
> load('./dtm.Rdata');
> 
> print('Preparing Data.')
[1] "Preparing Data."
> # Add prefix to feature names
> colnames(dtm)<-paste0('feature.',colnames(dtm))
> # Convert topic models to strings
> doc.topics<-sapply(topics(lda),as.character);
> 
> # Extract the purpose and tags column
> data<-data[,colnames(data) %in% c('purpose','tags')];
> # Combine Tags with features
> data<-cbind(data,doc.topics,dtm);
> 
> # Preparing class column
> colnames(data)[2]<-'class'; # Replace Class Name
> data$class<-sapply(data$class,as.character); # Convert to characters
> data$class<-factor(data$class); # Factorize class
> 
> # Convert to data frame
> data<-as.data.frame(data);
> 
> #Separate Training and Testing Set
> train_set<-subset(data,purpose == 'train'); # Get Training Set
> test_set<-subset(data,purpose == 'test'); # Get Training Set
> train_set<-train_set[,2:dim(data)[2]]; # Remove Purpose Column
> test_set<-test_set[,2:dim(data)[2]]; # Remove Purpose Column
> #Do cross validation
> K<-10;
> print(sprintf('Starting %d-fold cross validation',K));
[1] "Starting 10-fold cross validation"
> result<-cross_validation(train_set,method,K=K);
[1] "K-Fold: 1"
[1] "K-Fold: 2"
[1] "K-Fold: 3"
[1] "K-Fold: 4"
[1] "K-Fold: 5"
[1] "K-Fold: 6"
[1] "K-Fold: 7"
[1] "K-Fold: 8"
[1] "K-Fold: 9"
[1] "K-Fold: 10"
> confusionMatrices<-result$confusionMatrices;
> models<-result$models;
> 
> #Start Evaluation
> print('Evaluation on training set')
[1] "Evaluation on training set"
> #Do Micro Averaging
> overallConfusionMatrix<-confusionMatrices[[1]];
> for(i in 2:length(confusionMatrices)){
+ 	overallConfusionMatrix<-overallConfusionMatrix+confusionMatrices[[i]];
+ }
> accuracy<-sum(diag(overallConfusionMatrix))/sum(overallConfusionMatrix);
> performanceMatrix<-measure_performance(overallConfusionMatrix);
> print("Result of Micro-Averaging");
[1] "Result of Micro-Averaging"
> print(performanceMatrix);
      TP  FN  FP     Recall  Precision  F-measure
110  127  67  62 0.65463918 0.67195767 0.66318538
128  259  66  58 0.79692308 0.81703470 0.80685358
132    2 197  42 0.01005025 0.04545455 0.01646091
2   1399  84 204 0.94335806 0.87273862 0.90667531
21     2 158  43 0.01250000 0.04444444 0.01951220
30   257  87  55 0.74709302 0.82371795 0.78353659
37  2575 140 102 0.94843462 0.96189765 0.95511869
47   277 122 375 0.69423559 0.42484663 0.52711703
57   143 139  85 0.50709220 0.62719298 0.56078431
75   335 114 148 0.74610245 0.69358178 0.71888412
> print(get_overall_performance(performanceMatrix));
             Recall Precision F-measure
micro-avg 0.8207634 0.8207634 0.8207634
macro-avg 0.6060428 0.5982867 0.5958128
> print(sprintf("Accuracy:%f",accuracy));
[1] "Accuracy:0.820763"
> #Do Macro Averaging
> accuracies<-1:K*0;
> overallPerformanceMatrix<-measure_performance(confusionMatrices[[1]]);
> accuracies[1]<-sum(diag(confusionMatrices[[1]]))/sum(confusionMatrices[[1]]);
> for(i in 2:length(confusionMatrices)){
+ 	performanceMatrix<-measure_performance(confusionMatrices[[i]]);
+ 	overallPerformanceMatrix<-overallPerformanceMatrix+performanceMatrix;
+ 	accuracies[i]<-sum(diag(confusionMatrices[[i]]))/sum(confusionMatrices[[i]]);
+ }
> overallPerformanceMatrix[,4:6]<-overallPerformanceMatrix[,4:6]/K;
> print("Result of Macro-Averaing");
[1] "Result of Macro-Averaing"
> print(overallPerformanceMatrix);
      TP  FN  FP      Recall  Precision F-measure
110  127  67  62 0.651471858 0.66757943 0.6528496
128  259  66  58 0.795789172 0.82346717 0.8067608
132    2 197  42 0.009090909        NaN       NaN
2   1399  84 204 0.943617034 0.87328680 0.9068195
21     2 158  43 0.014583333 0.02678571       NaN
30   257  87  55 0.749475020 0.82127867 0.7824437
37  2575 140 102 0.948555079 0.96195668 0.9551276
47   277 122 375 0.697404970 0.42562528 0.5257387
57   143 139  85 0.507996812 0.63477968 0.5579297
75   335 114 148 0.747877852 0.69053231 0.7144749
> print(get_overall_performance(overallPerformanceMatrix));
             Recall Precision F-measure
micro-avg 0.8207634 0.8207634 0.8207634
macro-avg 0.6065862 0.6583657 0.7377681
> std_accuracy<-sqrt(var(accuracies));
> mean_accuracy<-mean(accuracies);
> print(sprintf("MeanAccuracy=%f",mean_accuracy));
[1] "MeanAccuracy=0.820763"
> print(sprintf("99%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-2.58*std_accuracy,
+ 	mean_accuracy+2.58*std_accuracy));
[1] "99% ConfidenceInterval[0.772052,0.869475]"
> print(sprintf("95%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.96*std_accuracy,
+ 	mean_accuracy+1.96*std_accuracy));
[1] "95% ConfidenceInterval[0.783758,0.857769]"
> print(sprintf("90%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.64*std_accuracy,
+ 	mean_accuracy+1.64*std_accuracy));
[1] "90% ConfidenceInterval[0.789800,0.851727]"
> 
> #Evaluation on test set
> print('Evaluation on test set')
[1] "Evaluation on test set"
> #Find best model
> accuracy_rank<-rank(-accuracies);
> best_model<-models[accuracy_rank==1][[1]];
> #Classification on test set
> prediction<-predict(best_model,test_set);
> #Construct Confusion Matrix
> confusionMatrix<-table(actual=test_set[,1],predicted=prediction);
> accuracy<-sum(diag(confusionMatrix))/sum(confusionMatrix);
> print(confusionMatrix)
      predicted
actual 110 128 132   2  21  30  37  47  57  75
   110  38   0   0  36   0  11   2   0   0   0
   128  22   1   0  54   4  13  14   0   0   0
   132  24   0   0  24  13   4   1   0   0   0
   2     4   0   0 587   0   0  53   0   0   0
   21   13   0   0  18   9   6   2   0   0   0
   30   16   0   0  93   0  28  27   0   0   0
   37    2   0   0  53   6   0 988   0   0   0
   47   42   0   0  49  20  14   9   0   0   0
   57    7   1   0  53   8   5  22   0   4   0
   75   11   0   0  85   7   7  30   0   0   0
> print(sprintf('Test Set: Accuracy=%f',accuracy))
[1] "Test Set: Accuracy=0.651575"
> 
> proc.time()
   user  system elapsed 
452.511   4.775 771.087 
