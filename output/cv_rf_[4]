
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(e1071);
> library(randomForest);
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
> library(topicmodels);
> #Normalize features of dataset
> normalize<-function(data){
+ 	cols<-ncol(data);
+ 	for(i in 1:cols){
+ 		if(is.numeric(data[,i]))data[,i]<-data[,i]/sum(data[,i]);
+ 	}
+ 	return(data);
+ }
> #Randomize the rows of dataset
> randomize<-function(data){
+ 	length<-nrow(data);
+ 	indices<-1:length;
+ 	rindices<-sample(indices,length,replace=F);
+ 	data<-data[rindices,];
+ 	return(data);
+ }
> #This function does cross-validation
> #input data	datamatrix
> #input method	randomForest,SVM,naiveBayes,etc
> #input K	K-foldvalidation
> #return		confusion matrices of each fold
> cross_validation<-function(data,method,K=10){
+ 	#Calculate matrix length and size
+ 	length<-nrow(data);
+ 	partitionSize<-floor(length/K);
+ 	#Confusion Matrix
+ 	confusionMatrices<-list();
+ 	#List of models
+ 	models<-list()
+ 	#Normalize Data
+ 	data<-normalize(data);
+ 	#Shuffle Data
+ 	randomData<-randomize(data);
+ 	#Perform Cross-Validation
+ 	for(k in 1:K){
+ 		# Debug
+ 		print(sprintf('K-Fold: %d',k));
+ 		#Get Traning Set
+ 		startIdx<-(k-1)*partitionSize+1;
+ 		indices<-startIdx:(startIdx+partitionSize-1);
+ 		trainset<-randomData[-indices,];
+ 		testset<-randomData[indices,];
+ 		#Train Data
+ 		ifelse(method=='svm',
+ 			model<-svm(class~.,data=trainset,kernel=svm.kernel,
+ 				gamma=svm.gamma,cost=svm.cost),
+ 			ifelse(method=='naiveBayes',
+ 				model<-naiveBayes(class~.,data=trainset),
+ 				model<-randomForest(class~.,data=trainset,mtry=rf.mtry)
+ 		));
+ 		prediction<-predict(model,testset);
+ 		#Construct Confusion Matrix
+ 		tab<-table(actual=testset[,1],predicted=prediction);
+ 		confusionMatrices[[k]]<-tab;
+ 		models[[k]]<-model;
+ 	}
+ 	return(list(confusionMatrices=confusionMatrices,models=models));
+ }
> #This function produce per class performance matrix from confusion matrix
> #input matrix confusionmatrix
> measure_performance<-function(mat){
+ 	#Get Size of Matrix
+ 	size<-nrow(mat);
+ 	#Initialize Result
+ 	result<-matrix(,nrow=size,ncol=6);
+ 	col_names<-c('TP','FN','FP','Recall','Precision','F-measure');
+ 	row_names<-rownames(mat);
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Iterate over all class
+ 	for(c in 1:size){
+ 	#Calculate Performance Metrics
+ 		TP<-mat[c,c];
+ 		FN<-sum(mat[c,-c]);
+ 		FP<-sum(mat[-c,c]);
+ 		recall<-TP/(TP+FN);
+ 		precision<-TP/(TP+FP);
+ 		fmeasure<-(2*precision*recall)/(precision+recall);
+ 		#SaveToResult
+ 		result[c,1]<-TP;result[c,2]<-FN;result[c,3]<-FP;
+ 		result[c,4]<-recall;result[c,5]<-precision;
+ 		result[c,6]<-fmeasure;
+ 	}
+ 	return(result);
+ }
> #This function produce the overall performance matrix
> #input matrix per class performance matrix
> #return over all performance matrix
> get_overall_performance<-function(mat){
+ 	result<-matrix(0,nrow=2,ncol=3);
+ 	col_names<-c('Recall','Precision','F-measure');
+ 	row_names<-c('micro-avg','macro-avg');
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Micro Averaging
+ 	TP<-sum(mat[,1]);
+ 	FN<-sum(mat[,2]);
+ 	FP<-sum(mat[,3]);
+ 	result[1,1]<-TP/(TP+FN);
+ 	result[1,2]<-TP/(TP+FP);
+ 	result[1,3]<-2*result[1,2]*result[1,1]/(result[1,1]+result[1,2]);
+ 	#Macro Averaging
+ 	result[2,1]<-mean(mat[,4],na.rm=T);
+ 	result[2,2]<-mean(mat[,5],na.rm=T);
+ 	result[2,3]<-mean(mat[,6],na.rm=T);
+ 	return(result);
+ }
> # Fetch Command Line Arguments
> args <- commandArgs(trailingOnly=T)
> ifelse(length(args) > 1, method<-args[1], method<-'naiveBayes')
[1] "randomForest"
> print(sprintf("Method: %s",method))
[1] "Method: randomForest"
> if(method == 'svm'){
+ 	if(length(args) == 4){
+ 		svm.kernel = args[2]
+ 		svm.gamma = as.numeric(args[3])
+ 		svm.cost = as.numeric(args[4])
+ 	}
+ 	else{
+ 		svm.kernel='radial'
+ 		svm.gamma = 0.01
+ 		svm.cost = 10000
+ 	}
+ 	print(sprintf("Settings: Kernel:%s Gamma:%f Cost:%f",
+ 		svm.kernel,svm.gamma,svm.cost))
+ }
> if(method == 'randomForest'){
+ 	if(length(args) == 2){
+ 		rf.mtry = as.numeric(args[2])
+ 	}
+ 	else{
+ 		rf.mtry = 4
+ 	}
+ 	print(sprintf("Settings: mtry:%f",rf.mtry))
+ }
[1] "Settings: mtry:4.000000"
> 
> #Prepare Data
> data<-read.csv('./doc_tags.csv',header=T);
> load('./lda.Rdata');
> load('./dtm.Rdata');
> 
> print('Preparing Data.')
[1] "Preparing Data."
> # Add prefix to feature names
> colnames(dtm)<-paste0('feature.',colnames(dtm))
> # Convert topic models to strings
> doc.topics<-sapply(topics(lda),as.character);
> 
> # Extract the purpose and tags column
> data<-data[,colnames(data) %in% c('purpose','tags')];
> # Combine Tags with features
> data<-cbind(data,doc.topics,dtm);
> 
> # Preparing class column
> colnames(data)[2]<-'class'; # Replace Class Name
> data$class<-sapply(data$class,as.character); # Convert to characters
> data$class<-factor(data$class); # Factorize class
> 
> # Convert to data frame
> data<-as.data.frame(data);
> 
> #Separate Training and Testing Set
> train_set<-subset(data,purpose == 'train'); # Get Training Set
> test_set<-subset(data,purpose == 'test'); # Get Training Set
> train_set<-train_set[,2:dim(data)[2]]; # Remove Purpose Column
> test_set<-test_set[,2:dim(data)[2]]; # Remove Purpose Column
> #Do cross validation
> K<-10;
> print(sprintf('Starting %d-fold cross validation',K));
[1] "Starting 10-fold cross validation"
> result<-cross_validation(train_set,method,K=K);
[1] "K-Fold: 1"
[1] "K-Fold: 2"
[1] "K-Fold: 3"
[1] "K-Fold: 4"
[1] "K-Fold: 5"
[1] "K-Fold: 6"
[1] "K-Fold: 7"
[1] "K-Fold: 8"
[1] "K-Fold: 9"
[1] "K-Fold: 10"
> confusionMatrices<-result$confusionMatrices;
> models<-result$models;
> 
> #Start Evaluation
> print('Evaluation on training set')
[1] "Evaluation on training set"
> #Do Micro Averaging
> overallConfusionMatrix<-confusionMatrices[[1]];
> for(i in 2:length(confusionMatrices)){
+ 	overallConfusionMatrix<-overallConfusionMatrix+confusionMatrices[[i]];
+ }
> accuracy<-sum(diag(overallConfusionMatrix))/sum(overallConfusionMatrix);
> performanceMatrix<-measure_performance(overallConfusionMatrix);
> print("Result of Micro-Averaging");
[1] "Result of Micro-Averaging"
> print(performanceMatrix);
      TP  FN  FP    Recall Precision F-measure
110   70 123  61 0.3626943 0.5343511 0.4320988
128  243  82  65 0.7476923 0.7889610 0.7677725
132    0 199 147 0.0000000 0.0000000       NaN
2   1425  61 311 0.9589502 0.8208525 0.8845438
21     0 161 119 0.0000000 0.0000000       NaN
30   229 115  74 0.6656977 0.7557756 0.7078825
37  2535 179  85 0.9340457 0.9675573 0.9505062
47   104 295 388 0.2606516 0.2113821 0.2334456
57    96 186  84 0.3404255 0.5333333 0.4155844
75   313 134 201 0.7002237 0.6089494 0.6514048
> print(get_overall_performance(performanceMatrix));
             Recall Precision F-measure
micro-avg 0.7656489 0.7656489 0.7656489
macro-avg 0.4970381 0.5221162 0.6304048
> print(sprintf("Accuracy:%f",accuracy));
[1] "Accuracy:0.765649"
> #Do Macro Averaging
> accuracies<-1:K*0;
> overallPerformanceMatrix<-measure_performance(confusionMatrices[[1]]);
> accuracies[1]<-sum(diag(confusionMatrices[[1]]))/sum(confusionMatrices[[1]]);
> for(i in 2:length(confusionMatrices)){
+ 	performanceMatrix<-measure_performance(confusionMatrices[[i]]);
+ 	overallPerformanceMatrix<-overallPerformanceMatrix+performanceMatrix;
+ 	accuracies[i]<-sum(diag(confusionMatrices[[i]]))/sum(confusionMatrices[[i]]);
+ }
> overallPerformanceMatrix[,4:6]<-overallPerformanceMatrix[,4:6]/K;
> print("Result of Macro-Averaing");
[1] "Result of Macro-Averaing"
> print(overallPerformanceMatrix);
      TP  FN  FP    Recall Precision F-measure
110   70 123  61 0.3647187 0.5456051 0.4277355
128  243  82  65 0.7474158 0.7858921 0.7631255
132    0 199 147 0.0000000 0.0000000       NaN
2   1425  61 311 0.9590987 0.8205668 0.8842247
21     0 161 119 0.0000000 0.0000000       NaN
30   229 115  74 0.6675152 0.7579496 0.7076701
37  2535 179  85 0.9338767 0.9672352 0.9501336
47   104 295 388 0.2623934 0.2140702 0.2342016
57    96 186  84 0.3328644 0.5204861 0.4041291
75   313 134 201 0.6990125 0.6090690 0.6503163
> print(get_overall_performance(overallPerformanceMatrix));
             Recall Precision F-measure
micro-avg 0.7656489 0.7656489 0.7656489
macro-avg 0.4966895 0.5220874 0.6276921
> std_accuracy<-sqrt(var(accuracies));
> mean_accuracy<-mean(accuracies);
> print(sprintf("MeanAccuracy=%f",mean_accuracy));
[1] "MeanAccuracy=0.765649"
> print(sprintf("99%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-2.58*std_accuracy,
+ 	mean_accuracy+2.58*std_accuracy));
[1] "99% ConfidenceInterval[0.721620,0.809678]"
> print(sprintf("95%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.96*std_accuracy,
+ 	mean_accuracy+1.96*std_accuracy));
[1] "95% ConfidenceInterval[0.732201,0.799097]"
> print(sprintf("90%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.64*std_accuracy,
+ 	mean_accuracy+1.64*std_accuracy));
[1] "90% ConfidenceInterval[0.737662,0.793636]"
> 
> #Evaluation on test set
> print('Evaluation on test set')
[1] "Evaluation on test set"
> #Find best model
> accuracy_rank<-rank(-accuracies);
> best_model<-models[accuracy_rank==1][[1]];
> #Classification on test set
> prediction<-predict(best_model,test_set);
> #Construct Confusion Matrix
> confusionMatrix<-table(actual=test_set[,1],predicted=prediction);
> accuracy<-sum(diag(confusionMatrix))/sum(confusionMatrix);
> print(confusionMatrix)
      predicted
actual  110  128  132    2   21   30   37   47   57   75
   110   51    2    0    3    0   22    3    5    0    1
   128    0   95    0    3    0    1    0    3    1    5
   132    0    2    0    2    0    0    1   61    0    0
   2      0    3    0  619    0    5   16    0    0    1
   21     0    2    0    2    0    0    0   43    1    0
   30    18    2    0   13    0  126    3    0    0    2
   37     0    0    0   36    0    1 1012    0    0    0
   47     4   10    0    4    0    1    3  110    1    1
   57     0    4    0   12    0    0    1    1   40   42
   75     0    3    0    9    0    0    0    0   19  109
> print(sprintf('Test Set: Accuracy=%f',accuracy))
[1] "Test Set: Accuracy=0.851181"
> 
> proc.time()
    user   system  elapsed 
1308.046   13.049 2102.911 
