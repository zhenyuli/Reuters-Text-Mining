
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(e1071);
> library(randomForest);
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
> library(topicmodels);
> #Normalize features of dataset
> normalize<-function(data){
+ 	cols<-ncol(data);
+ 	for(i in 1:cols){
+ 		if(is.numeric(data[,i]))data[,i]<-data[,i]/sum(data[,i]);
+ 	}
+ 	return(data);
+ }
> #Randomize the rows of dataset
> randomize<-function(data){
+ 	length<-nrow(data);
+ 	indices<-1:length;
+ 	rindices<-sample(indices,length,replace=F);
+ 	data<-data[rindices,];
+ 	return(data);
+ }
> #This function does cross-validation
> #input data	datamatrix
> #input method	randomForest,SVM,naiveBayes,etc
> #input K	K-foldvalidation
> #return		confusion matrices of each fold
> cross_validation<-function(data,method,K=10){
+ 	#Calculate matrix length and size
+ 	length<-nrow(data);
+ 	partitionSize<-floor(length/K);
+ 	#Confusion Matrix
+ 	confusionMatrices<-list();
+ 	#List of models
+ 	models<-list()
+ 	#Normalize Data
+ 	data<-normalize(data);
+ 	#Shuffle Data
+ 	randomData<-randomize(data);
+ 	#Perform Cross-Validation
+ 	for(k in 1:K){
+ 		# Debug
+ 		print(sprintf('K-Fold: %d',k));
+ 		#Get Traning Set
+ 		startIdx<-(k-1)*partitionSize+1;
+ 		indices<-startIdx:(startIdx+partitionSize-1);
+ 		trainset<-randomData[-indices,];
+ 		testset<-randomData[indices,];
+ 		#Train Data
+ 		ifelse(method=='svm',
+ 			model<-svm(class~.,data=trainset,kernel=svm.kernel,
+ 				gamma=svm.gamma,cost=svm.cost),
+ 			ifelse(method=='naiveBayes',
+ 				model<-naiveBayes(class~.,data=trainset),
+ 				model<-randomForest(class~.,data=trainset,mtry=rf.mtry)
+ 		));
+ 		prediction<-predict(model,testset);
+ 		#Construct Confusion Matrix
+ 		tab<-table(actual=testset[,1],predicted=prediction);
+ 		confusionMatrices[[k]]<-tab;
+ 		models[[k]]<-model;
+ 	}
+ 	return(list(confusionMatrices=confusionMatrices,models=models));
+ }
> #This function produce per class performance matrix from confusion matrix
> #input matrix confusionmatrix
> measure_performance<-function(mat){
+ 	#Get Size of Matrix
+ 	size<-nrow(mat);
+ 	#Initialize Result
+ 	result<-matrix(,nrow=size,ncol=6);
+ 	col_names<-c('TP','FN','FP','Recall','Precision','F-measure');
+ 	row_names<-rownames(mat);
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Iterate over all class
+ 	for(c in 1:size){
+ 	#Calculate Performance Metrics
+ 		TP<-mat[c,c];
+ 		FN<-sum(mat[c,-c]);
+ 		FP<-sum(mat[-c,c]);
+ 		recall<-TP/(TP+FN);
+ 		precision<-TP/(TP+FP);
+ 		fmeasure<-(2*precision*recall)/(precision+recall);
+ 		#SaveToResult
+ 		result[c,1]<-TP;result[c,2]<-FN;result[c,3]<-FP;
+ 		result[c,4]<-recall;result[c,5]<-precision;
+ 		result[c,6]<-fmeasure;
+ 	}
+ 	return(result);
+ }
> #This function produce the overall performance matrix
> #input matrix per class performance matrix
> #return over all performance matrix
> get_overall_performance<-function(mat){
+ 	result<-matrix(0,nrow=2,ncol=3);
+ 	col_names<-c('Recall','Precision','F-measure');
+ 	row_names<-c('micro-avg','macro-avg');
+ 	colnames(result)<-col_names;
+ 	rownames(result)<-row_names;
+ 	#Micro Averaging
+ 	TP<-sum(mat[,1]);
+ 	FN<-sum(mat[,2]);
+ 	FP<-sum(mat[,3]);
+ 	result[1,1]<-TP/(TP+FN);
+ 	result[1,2]<-TP/(TP+FP);
+ 	result[1,3]<-2*result[1,2]*result[1,1]/(result[1,1]+result[1,2]);
+ 	#Macro Averaging
+ 	result[2,1]<-mean(mat[,4],na.rm=T);
+ 	result[2,2]<-mean(mat[,5],na.rm=T);
+ 	result[2,3]<-mean(mat[,6],na.rm=T);
+ 	return(result);
+ }
> # Fetch Command Line Arguments
> args <- commandArgs(trailingOnly=T)
> ifelse(length(args) > 1, method<-args[1], method<-'naiveBayes')
[1] "svm"
> print(sprintf("Method: %s",method))
[1] "Method: svm"
> if(method == 'svm'){
+ 	if(length(args) == 4){
+ 		svm.kernel = args[2]
+ 		svm.gamma = as.numeric(args[3])
+ 		svm.cost = as.numeric(args[4])
+ 	}
+ 	else{
+ 		svm.kernel='radial'
+ 		svm.gamma = 0.01
+ 		svm.cost = 10000
+ 	}
+ 	print(sprintf("Settings: Kernel:%s Gamma:%f Cost:%f",
+ 		svm.kernel,svm.gamma,svm.cost))
+ }
[1] "Settings: Kernel:linear Gamma:0.100000 Cost:0.001000"
> if(method == 'randomForest'){
+ 	if(length(args) == 2){
+ 		rf.mtry = as.numeric(args[2])
+ 	}
+ 	else{
+ 		rf.mtry = 4
+ 	}
+ 	print(sprintf("Settings: mtry:%f",rf.mtry))
+ }
> 
> #Prepare Data
> data<-read.csv('./doc_tags.csv',header=T);
> load('./lda.Rdata');
> load('./dtm.Rdata');
> 
> print('Preparing Data.')
[1] "Preparing Data."
> # Add prefix to feature names
> colnames(dtm)<-paste0('feature.',colnames(dtm))
> # Convert topic models to strings
> doc.topics<-sapply(topics(lda),as.character);
> 
> # Extract the purpose and tags column
> data<-data[,colnames(data) %in% c('purpose','tags')];
> # Combine Tags with features
> data<-cbind(data,doc.topics,dtm);
> 
> # Preparing class column
> colnames(data)[2]<-'class'; # Replace Class Name
> data$class<-sapply(data$class,as.character); # Convert to characters
> data$class<-factor(data$class); # Factorize class
> 
> # Convert to data frame
> data<-as.data.frame(data);
> 
> #Separate Training and Testing Set
> train_set<-subset(data,purpose == 'train'); # Get Training Set
> test_set<-subset(data,purpose == 'test'); # Get Training Set
> train_set<-train_set[,2:dim(data)[2]]; # Remove Purpose Column
> test_set<-test_set[,2:dim(data)[2]]; # Remove Purpose Column
> #Do cross validation
> K<-10;
> print(sprintf('Starting %d-fold cross validation',K));
[1] "Starting 10-fold cross validation"
> result<-cross_validation(train_set,method,K=K);
[1] "K-Fold: 1"
[1] "K-Fold: 2"
[1] "K-Fold: 3"
[1] "K-Fold: 4"
[1] "K-Fold: 5"
[1] "K-Fold: 6"
[1] "K-Fold: 7"
[1] "K-Fold: 8"
[1] "K-Fold: 9"
[1] "K-Fold: 10"
> confusionMatrices<-result$confusionMatrices;
> models<-result$models;
> 
> #Start Evaluation
> print('Evaluation on training set')
[1] "Evaluation on training set"
> #Do Micro Averaging
> overallConfusionMatrix<-confusionMatrices[[1]];
> for(i in 2:length(confusionMatrices)){
+ 	overallConfusionMatrix<-overallConfusionMatrix+confusionMatrices[[i]];
+ }
> accuracy<-sum(diag(overallConfusionMatrix))/sum(overallConfusionMatrix);
> performanceMatrix<-measure_performance(overallConfusionMatrix);
> print("Result of Micro-Averaging");
[1] "Result of Micro-Averaging"
> print(performanceMatrix);
      TP  FN  FP    Recall Precision F-measure
110  110  84  46 0.5670103 0.7051282 0.6285714
128  244  81  62 0.7507692 0.7973856 0.7733756
132    0 199   1 0.0000000 0.0000000       NaN
2   1411  72 315 0.9514498 0.8174971 0.8794017
21     0 161   7 0.0000000 0.0000000       NaN
30   232 112  48 0.6744186 0.8285714 0.7435897
37  2546 168 245 0.9380987 0.9122178 0.9249773
47   321  78 345 0.8045113 0.4819820 0.6028169
57   100 182  50 0.3546099 0.6666667 0.4629630
75   324 125 143 0.7216036 0.6937901 0.7074236
> print(get_overall_performance(performanceMatrix));
             Recall Precision F-measure
micro-avg 0.8073282 0.8073282 0.8073282
macro-avg 0.5762471 0.5903239 0.7153899
> print(sprintf("Accuracy:%f",accuracy));
[1] "Accuracy:0.807328"
> #Do Macro Averaging
> accuracies<-1:K*0;
> overallPerformanceMatrix<-measure_performance(confusionMatrices[[1]]);
> accuracies[1]<-sum(diag(confusionMatrices[[1]]))/sum(confusionMatrices[[1]]);
> for(i in 2:length(confusionMatrices)){
+ 	performanceMatrix<-measure_performance(confusionMatrices[[i]]);
+ 	overallPerformanceMatrix<-overallPerformanceMatrix+performanceMatrix;
+ 	accuracies[i]<-sum(diag(confusionMatrices[[i]]))/sum(confusionMatrices[[i]]);
+ }
> overallPerformanceMatrix[,4:6]<-overallPerformanceMatrix[,4:6]/K;
> print("Result of Macro-Averaing");
[1] "Result of Macro-Averaing"
> print(overallPerformanceMatrix);
      TP  FN  FP    Recall Precision F-measure
110  110  84  46 0.5692179 0.7121526 0.6251783
128  244  81  62 0.7478215 0.7960994 0.7689917
132    0 199   1 0.0000000       NaN       NaN
2   1411  72 315 0.9514314 0.8167997 0.8787289
21     0 161   7 0.0000000       NaN       NaN
30   232 112  48 0.6746713 0.8319138 0.7403317
37  2546 168 245 0.9384573 0.9121596 0.9249857
47   321  78 345 0.8069272 0.4821487 0.6020895
57   100 182  50 0.3527339 0.6740138 0.4541787
75   324 125 143 0.7221189 0.6996070 0.7073098
> print(get_overall_performance(overallPerformanceMatrix));
             Recall Precision F-measure
micro-avg 0.8073282 0.8073282 0.8073282
macro-avg 0.5763380 0.7406118 0.7127243
> std_accuracy<-sqrt(var(accuracies));
> mean_accuracy<-mean(accuracies);
> print(sprintf("MeanAccuracy=%f",mean_accuracy));
[1] "MeanAccuracy=0.807328"
> print(sprintf("99%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-2.58*std_accuracy,
+ 	mean_accuracy+2.58*std_accuracy));
[1] "99% ConfidenceInterval[0.773607,0.841049]"
> print(sprintf("95%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.96*std_accuracy,
+ 	mean_accuracy+1.96*std_accuracy));
[1] "95% ConfidenceInterval[0.781711,0.832946]"
> print(sprintf("90%% ConfidenceInterval[%f,%f]",
+ 	mean_accuracy-1.64*std_accuracy,
+ 	mean_accuracy+1.64*std_accuracy));
[1] "90% ConfidenceInterval[0.785893,0.828763]"
> 
> #Evaluation on test set
> print('Evaluation on test set')
[1] "Evaluation on test set"
> #Find best model
> accuracy_rank<-rank(-accuracies);
> best_model<-models[accuracy_rank==1][[1]];
> #Classification on test set
> prediction<-predict(best_model,test_set);
> #Construct Confusion Matrix
> confusionMatrix<-table(actual=test_set[,1],predicted=prediction);
> accuracy<-sum(diag(confusionMatrix))/sum(confusionMatrix);
> print(confusionMatrix)
      predicted
actual 110 128 132   2  21  30  37  47  57  75
   110   7   0   0  11   0  68   1   0   0   0
   128   0  23   0  19   3  60   3   0   0   0
   132   8   2   5   3  18  30   0   0   0   0
   2     1   0   0 595   0  27  21   0   0   0
   21    6   0   1   5  14  22   0   0   0   0
   30    1   0   0  34   0 127   2   0   0   0
   37   10   0   0  71   2  14 952   0   0   0
   47   18   2   5  12  28  69   0   0   0   0
   57    2   2   0  26   1  57   3   0   9   0
   75    1   0   0  43   0  90   3   0   2   1
> print(sprintf('Test Set: Accuracy=%f',accuracy))
[1] "Test Set: Accuracy=0.682283"
> 
> proc.time()
    user   system  elapsed 
 658.025    4.766 1069.438 
